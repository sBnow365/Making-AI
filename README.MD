# ğŸ§  OCR Evaluation Tool

This project is a comprehensive **OCR (Optical Character Recognition) Evaluation Framework** designed to rigorously test and analyze the performance of custom OCR pipelines. It supports printed and handwritten text recognition, and delivers detailed performance insights including character-level metrics, word accuracy, and confusion matrices.

---

## ğŸ“ Project Structure

- `dataset/img/` â€“ Folder containing input images for OCR evaluation  
- `dataset/text/` â€“ Corresponding ground truth `.txt` files  
- `backend.py` â€“ Includes `extract_text()` for OCR inference and `preprocess_image()` for image preparation  
- `evaluate.py` â€“ Main evaluation script computing metrics and visualizations

---

## ğŸ“Š Metrics Computed

- âœ… **Word-Level Accuracy**
- ğŸ”  **Character-Level Accuracy**
- âœï¸ **Average Levenshtein Distance**
- ğŸ¯ **Precision, Recall, and F1 Score (char-wise)**
- ğŸ” **Character Confusion Matrix** via Seaborn heatmap

---

## ğŸ› ï¸ How It Works

For each image in `dataset/img/`, the following is performed:

1. Ground truth is loaded from the corresponding `.txt` file.
2. The image is preprocessed using `preprocess_image`.
3. Text is extracted using a custom OCR engine via `extract_text`.
4. Evaluation metrics are computed:
   - Word-wise comparison
   - Character-wise match count
   - Levenshtein distance
   - Confusion matrix data

Results are accumulated and visualized in real-time.

---

## ğŸ–¥ï¸ Sample Output (Console)

